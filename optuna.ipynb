{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "817c62e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\abhyu\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import optuna\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from sklearn.utils import shuffle\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3e3b683",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-09 18:40:47,529] A new study created in memory with name: no-name-78cfc9c2-b379-4f76-bc17-29ea269b66a0\n",
      "Best trial: 1. Best value: 0.90216:   4%|‚ñç         | 1/25 [00:00<00:11,  2.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-11-09 18:40:48,022] Trial 0 finished with value: 0.6978998384491115 and parameters: {'model': 'RandomForest', 'n_estimators': 212, 'max_depth': 16, 'min_samples_split': 5}. Best is trial 0 with value: 0.6978998384491115.\n",
      "[I 2025-11-09 18:40:48,029] Trial 1 finished with value: 0.9021601016518425 and parameters: {'model': 'NaiveBayes', 'alpha': 0.1}. Best is trial 1 with value: 0.9021601016518425.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 2. Best value: 0.902724:  12%|‚ñà‚ñè        | 3/25 [00:08<01:09,  3.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-11-09 18:40:56,234] Trial 2 finished with value: 0.9027237354085603 and parameters: {'model': 'XGBoost', 'n_estimators': 408, 'max_depth': 5, 'learning_rate': 0.04632537647154444, 'subsample': 0.6581426803566224}. Best is trial 2 with value: 0.9027237354085603.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 4. Best value: 0.904824:  16%|‚ñà‚ñå        | 4/25 [00:09<00:45,  2.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-11-09 18:40:56,549] Trial 3 finished with value: 0.28205128205128205 and parameters: {'model': 'RandomForest', 'n_estimators': 138, 'max_depth': 7, 'min_samples_split': 9}. Best is trial 2 with value: 0.9027237354085603.\n",
      "[I 2025-11-09 18:40:56,558] Trial 4 finished with value: 0.9048239895697523 and parameters: {'model': 'NaiveBayes', 'alpha': 1.0}. Best is trial 4 with value: 0.9048239895697523.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 6. Best value: 0.911652:  24%|‚ñà‚ñà‚ñç       | 6/25 [00:09<00:23,  1.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-11-09 18:40:57,121] Trial 5 finished with value: 0.7083333333333334 and parameters: {'model': 'RandomForest', 'n_estimators': 249, 'max_depth': 17, 'min_samples_split': 8}. Best is trial 4 with value: 0.9048239895697523.\n",
      "[I 2025-11-09 18:40:57,128] Trial 6 finished with value: 0.911651728553137 and parameters: {'model': 'NaiveBayes', 'alpha': 0.5}. Best is trial 6 with value: 0.911651728553137.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 7. Best value: 0.915:  36%|‚ñà‚ñà‚ñà‚ñå      | 9/25 [00:21<00:51,  3.24s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-11-09 18:41:09,221] Trial 7 finished with value: 0.915 and parameters: {'model': 'XGBoost', 'n_estimators': 403, 'max_depth': 10, 'learning_rate': 0.12924366441412957, 'subsample': 0.6988389061009636}. Best is trial 7 with value: 0.915.\n",
      "[I 2025-11-09 18:41:09,235] Trial 8 finished with value: 0.8959156785243741 and parameters: {'model': 'NaiveBayes', 'alpha': 1.5000000000000002}. Best is trial 7 with value: 0.915.\n",
      "[I 2025-11-09 18:41:09,251] Trial 9 finished with value: 0.8941798941798942 and parameters: {'model': 'NaiveBayes', 'alpha': 1.7000000000000002}. Best is trial 7 with value: 0.915.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 7. Best value: 0.915:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 11/25 [00:33<00:50,  3.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-11-09 18:41:21,303] Trial 10 finished with value: 0.9072681704260651 and parameters: {'model': 'XGBoost', 'n_estimators': 489, 'max_depth': 9, 'learning_rate': 0.23009828749516437, 'subsample': 0.8825255796823629}. Best is trial 7 with value: 0.915.\n",
      "[I 2025-11-09 18:41:21,351] Trial 11 finished with value: 0.0 and parameters: {'model': 'LogisticRegression', 'C': 0.025174969210228212, 'penalty': 'l2'}. Best is trial 7 with value: 0.915.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 13. Best value: 0.92132:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 16/25 [00:43<00:21,  2.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-11-09 18:41:30,687] Trial 12 finished with value: 0.9086357947434293 and parameters: {'model': 'XGBoost', 'n_estimators': 360, 'max_depth': 9, 'learning_rate': 0.18906229807767225, 'subsample': 0.5039001902905458}. Best is trial 7 with value: 0.915.\n",
      "[I 2025-11-09 18:41:30,739] Trial 13 finished with value: 0.9213197969543148 and parameters: {'model': 'LogisticRegression', 'C': 8.973975056762294, 'penalty': 'l2'}. Best is trial 13 with value: 0.9213197969543148.\n",
      "[I 2025-11-09 18:41:30,787] Trial 14 finished with value: 0.9185750636132316 and parameters: {'model': 'LogisticRegression', 'C': 8.26667590009963, 'penalty': 'l2'}. Best is trial 13 with value: 0.9213197969543148.\n",
      "[I 2025-11-09 18:41:30,831] Trial 15 finished with value: 0.9185750636132316 and parameters: {'model': 'LogisticRegression', 'C': 8.503586720080055, 'penalty': 'l2'}. Best is trial 13 with value: 0.9213197969543148.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 13. Best value: 0.92132:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 19/25 [00:43<00:09,  1.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-11-09 18:41:30,882] Trial 16 finished with value: 0.9185750636132316 and parameters: {'model': 'LogisticRegression', 'C': 9.885681105961336, 'penalty': 'l2'}. Best is trial 13 with value: 0.9213197969543148.\n",
      "[I 2025-11-09 18:41:30,925] Trial 17 finished with value: 0.8906455862977603 and parameters: {'model': 'LogisticRegression', 'C': 1.0545173620612953, 'penalty': 'l2'}. Best is trial 13 with value: 0.9213197969543148.\n",
      "[I 2025-11-09 18:41:30,963] Trial 18 finished with value: 0.8894736842105263 and parameters: {'model': 'LogisticRegression', 'C': 1.0632486495935003, 'penalty': 'l2'}. Best is trial 13 with value: 0.9213197969543148.\n",
      "[I 2025-11-09 18:41:31,023] Trial 19 finished with value: 0.9097938144329897 and parameters: {'model': 'LogisticRegression', 'C': 3.0158185893473703, 'penalty': 'l2'}. Best is trial 13 with value: 0.9213197969543148.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 13. Best value: 0.92132:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 24/25 [00:43<00:00,  1.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-11-09 18:41:31,078] Trial 20 finished with value: 0.73125 and parameters: {'model': 'LogisticRegression', 'C': 0.17840388716277414, 'penalty': 'l2'}. Best is trial 13 with value: 0.9213197969543148.\n",
      "[I 2025-11-09 18:41:31,133] Trial 21 finished with value: 0.9199491740787802 and parameters: {'model': 'LogisticRegression', 'C': 8.824976519480057, 'penalty': 'l2'}. Best is trial 13 with value: 0.9213197969543148.\n",
      "[I 2025-11-09 18:41:31,182] Trial 22 finished with value: 0.9213197969543148 and parameters: {'model': 'LogisticRegression', 'C': 9.170153637892644, 'penalty': 'l2'}. Best is trial 13 with value: 0.9213197969543148.\n",
      "[I 2025-11-09 18:41:31,226] Trial 23 finished with value: 0.9097938144329897 and parameters: {'model': 'LogisticRegression', 'C': 2.7255924668800877, 'penalty': 'l2'}. Best is trial 13 with value: 0.9213197969543148.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 13. Best value: 0.92132: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 25/25 [00:43<00:00,  1.75s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-11-09 18:41:31,283] Trial 24 finished with value: 0.9109677419354839 and parameters: {'model': 'LogisticRegression', 'C': 2.8914710650568756, 'penalty': 'l2'}. Best is trial 13 with value: 0.9213197969543148.\n",
      "\n",
      "üèÅ Optimization finished!\n",
      "Best Model: LogisticRegression\n",
      "Best Parameters: {'model': 'LogisticRegression', 'C': 8.973975056762294, 'penalty': 'l2'}\n",
      "Best F1 Score: 0.9213\n",
      "‚úÖ Best model retrained successfully!\n",
      "üíæ Best model and vectorizer saved successfully!\n"
     ]
    }
   ],
   "source": [
    "# =========================================\n",
    "# 1Ô∏è‚É£ Load Cleaned Dataset\n",
    "# =========================================\n",
    "df = pd.read_csv(\"cleaned_spam_dataset.csv\")\n",
    "\n",
    "# Ensure no missing text\n",
    "df[\"clean_text\"] = df[\"clean_text\"].astype(str).fillna(\"\")\n",
    "df = shuffle(df, random_state=42)\n",
    "\n",
    "X = df[\"clean_text\"]\n",
    "y = df[\"spam\"]\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# TF-IDF vectorization\n",
    "vectorizer = TfidfVectorizer(max_features=3000, stop_words=\"english\")\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = vectorizer.transform(X_test)\n",
    "\n",
    "# =========================================\n",
    "# 2Ô∏è‚É£ Define Objective Function for Optuna\n",
    "# =========================================\n",
    "def objective(trial):\n",
    "    model_name = trial.suggest_categorical(\n",
    "        \"model\", [\"NaiveBayes\", \"LogisticRegression\", \"RandomForest\", \"XGBoost\"]\n",
    "    )\n",
    "    \n",
    "    if model_name == \"NaiveBayes\":\n",
    "        alpha = trial.suggest_float(\"alpha\", 0.1, 2.0, step=0.1)\n",
    "        model = MultinomialNB(alpha=alpha)\n",
    "    \n",
    "    elif model_name == \"LogisticRegression\":\n",
    "        c = trial.suggest_float(\"C\", 0.01, 10.0, log=True)\n",
    "        penalty = trial.suggest_categorical(\"penalty\", [\"l2\"])\n",
    "        model = LogisticRegression(C=c, penalty=penalty, solver=\"lbfgs\", max_iter=2000)\n",
    "    \n",
    "    elif model_name == \"RandomForest\":\n",
    "        n_estimators = trial.suggest_int(\"n_estimators\", 100, 500)\n",
    "        max_depth = trial.suggest_int(\"max_depth\", 5, 30)\n",
    "        min_samples_split = trial.suggest_int(\"min_samples_split\", 2, 10)\n",
    "        model = RandomForestClassifier(\n",
    "            n_estimators=n_estimators,\n",
    "            max_depth=max_depth,\n",
    "            min_samples_split=min_samples_split,\n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "    \n",
    "    elif model_name == \"XGBoost\":\n",
    "        n_estimators = trial.suggest_int(\"n_estimators\", 100, 500)\n",
    "        max_depth = trial.suggest_int(\"max_depth\", 3, 10)\n",
    "        learning_rate = trial.suggest_float(\"learning_rate\", 0.01, 0.3, log=True)\n",
    "        subsample = trial.suggest_float(\"subsample\", 0.5, 1.0)\n",
    "        model = XGBClassifier(\n",
    "            n_estimators=n_estimators,\n",
    "            max_depth=max_depth,\n",
    "            learning_rate=learning_rate,\n",
    "            subsample=subsample,\n",
    "            eval_metric=\"logloss\",\n",
    "            use_label_encoder=False,\n",
    "            random_state=42\n",
    "        )\n",
    "\n",
    "    # Train and evaluate\n",
    "    model.fit(X_train_tfidf, y_train)\n",
    "    y_pred = model.predict(X_test_tfidf)\n",
    "\n",
    "    # We‚Äôll maximize F1-score (you can change to accuracy if you want)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    return f1\n",
    "\n",
    "\n",
    "# =========================================\n",
    "# 3Ô∏è‚É£ Run Optuna Optimization\n",
    "# =========================================\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=25, show_progress_bar=True)\n",
    "\n",
    "# =========================================\n",
    "# 4Ô∏è‚É£ Results\n",
    "# =========================================\n",
    "print(\"\\nüèÅ Optimization finished!\")\n",
    "print(\"Best Model:\", study.best_params[\"model\"])\n",
    "print(\"Best Parameters:\", study.best_params)\n",
    "print(\"Best F1 Score:\", round(study.best_value, 4))\n",
    "\n",
    "# =========================================\n",
    "# 5Ô∏è‚É£ Retrain Best Model on Full Dataset\n",
    "# =========================================\n",
    "best_params = study.best_params\n",
    "best_model_name = best_params[\"model\"]\n",
    "\n",
    "if best_model_name == \"NaiveBayes\":\n",
    "    model = MultinomialNB(alpha=best_params[\"alpha\"])\n",
    "\n",
    "elif best_model_name == \"LogisticRegression\":\n",
    "    model = LogisticRegression(C=best_params[\"C\"], penalty=\"l2\", solver=\"lbfgs\", max_iter=2000)\n",
    "\n",
    "elif best_model_name == \"RandomForest\":\n",
    "    model = RandomForestClassifier(\n",
    "        n_estimators=best_params[\"n_estimators\"],\n",
    "        max_depth=best_params[\"max_depth\"],\n",
    "        min_samples_split=best_params[\"min_samples_split\"],\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "elif best_model_name == \"XGBoost\":\n",
    "    model = XGBClassifier(\n",
    "        n_estimators=best_params[\"n_estimators\"],\n",
    "        max_depth=best_params[\"max_depth\"],\n",
    "        learning_rate=best_params[\"learning_rate\"],\n",
    "        subsample=best_params[\"subsample\"],\n",
    "        eval_metric=\"logloss\",\n",
    "        use_label_encoder=False,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "model.fit(X_train_tfidf, y_train)\n",
    "print(\"‚úÖ Best model retrained successfully!\")\n",
    "\n",
    "# =========================================\n",
    "# 6Ô∏è‚É£ Save Best Model and Vectorizer\n",
    "# =========================================\n",
    "with open(\"best_spam_model.pkl\", \"wb\") as f:\n",
    "    pickle.dump(model, f)\n",
    "\n",
    "with open(\"best_vectorizer.pkl\", \"wb\") as f:\n",
    "    pickle.dump(vectorizer, f)\n",
    "\n",
    "print(\"üíæ Best model and vectorizer saved successfully!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
