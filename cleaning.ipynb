{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f270735d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\abhyu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\abhyu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Datasets loaded:\n",
      "emails.csv â†’ (5728, 2)\n",
      "spam.csv â†’ (5572, 2)\n",
      "âœ… Combined dataset shape: (10852, 2)\n",
      "\n",
      "ğŸ” Before vs After Cleaning:\n",
      "\n",
      "ğŸ“© Original: Subject: naturally irresistible your corporate identity  lt is really hard to recollect a company : \n",
      "ğŸ§¼ Cleaned : naturally irresistible corporate identity lt really hard recollect company market full suqgestions i\n",
      "\n",
      "ğŸ“© Original: Subject: the stock trading gunslinger  fanny is merrill but muzo not colza attainder and penultimate\n",
      "ğŸ§¼ Cleaned : stock trading gunslinger fanny merrill muzo colza attainder penultimate like esmark perspicuous ramb\n",
      "\n",
      "ğŸ“© Original: Subject: unbelievable new homes made easy  im wanting to show you this  homeowner  you have been pre\n",
      "ğŸ§¼ Cleaned : unbelievable new home made easy im wanting show homeowner pre approved home loan fixed rate offer ex\n",
      "\n",
      "ğŸ’¾ Saved cleaned dataset as 'cleaned_spam_dataset.csv'\n",
      "âœ… Cleaning complete!\n"
     ]
    }
   ],
   "source": [
    "# ===============================\n",
    "# ğŸ§¹ SPAM DETECTION â€” DATA CLEANING\n",
    "# ===============================\n",
    "\n",
    "# ğŸ“¦ Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# ===============================\n",
    "# ğŸ“‚ 1. Load Datasets\n",
    "# ===============================\n",
    "df1 = pd.read_csv(\"datasets/emails.csv\")\n",
    "df2 = pd.read_csv(\"datasets/spam.csv\")\n",
    "\n",
    "print(\"âœ… Datasets loaded:\")\n",
    "print(\"emails.csv â†’\", df1.shape)\n",
    "print(\"spam.csv â†’\", df2.shape)\n",
    "\n",
    "# ===============================\n",
    "# ğŸ§¾ 2. Basic Cleaning\n",
    "# ===============================\n",
    "df2 = df2.rename(columns={\"Category\": \"spam\", \"Message\": \"text\"})\n",
    "df2[\"spam\"] = df2[\"spam\"].map({\"spam\": 1, \"ham\": 0})\n",
    "\n",
    "df = pd.concat([df1, df2], ignore_index=True)\n",
    "df.drop_duplicates(subset=\"text\", inplace=True)\n",
    "df.dropna(subset=[\"text\"], inplace=True)\n",
    "print(\"âœ… Combined dataset shape:\", df.shape)\n",
    "\n",
    "# ===============================\n",
    "# ğŸ§½ 3. Advanced Cleaning Function\n",
    "# ===============================\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def clean_text(text):\n",
    "    text = str(text).lower()\n",
    "    \n",
    "    # --- Remove unwanted prefixes ---\n",
    "    text = re.sub(r'^\\s*(re|fwd|fw|subject)\\s*:\\s*', '', text, flags=re.IGNORECASE)\n",
    "    \n",
    "    # --- Remove URLs and HTML ---\n",
    "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", '', text)\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    \n",
    "    # --- Remove digits and punctuation ---\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    \n",
    "    # --- Remove extra spaces ---\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    # --- Remove stopwords and lemmatize ---\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in text.split() if word not in stop_words]\n",
    "    text = \" \".join(tokens)\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Apply cleaning\n",
    "df[\"clean_text\"] = df[\"text\"].apply(clean_text)\n",
    "\n",
    "# ===============================\n",
    "# ğŸ§  4. Verify Cleaning\n",
    "# ===============================\n",
    "print(\"\\nğŸ” Before vs After Cleaning:\")\n",
    "for i in range(3):\n",
    "    print(f\"\\nğŸ“© Original: {df['text'].iloc[i][:100]}\")\n",
    "    print(f\"ğŸ§¼ Cleaned : {df['clean_text'].iloc[i][:100]}\")\n",
    "\n",
    "# ===============================\n",
    "# ğŸ’¾ 5. Save Cleaned Dataset\n",
    "# ===============================\n",
    "df.to_csv(\"cleaned_spam_dataset.csv\", index=False)\n",
    "print(\"\\nğŸ’¾ Saved cleaned dataset as 'cleaned_spam_dataset.csv'\")\n",
    "print(\"âœ… Cleaning complete!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c77e5c0a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
